#!/usr/bin/env bash
# Repository Inventory Hunter
# Purpose: Provide an upstream, system-level view of repository file distribution,
# classification, and cleanup / consolidation opportunities.
# Output: JSON report + Markdown summary with actionable heuristics.

set -euo pipefail

OUT_DIR="__reports/hunt"
JSON_OUT="$OUT_DIR/repo_inventory.json"
MD_OUT="$OUT_DIR/repo_inventory.md"

mkdir -p "$OUT_DIR"

root_dir="${1:-.}"

# Exclusions (augment as needed)
EXCLUDE_DIRS_REGEX='(^|/)(node_modules|dist|.git|__reports/hunt|__tmp|tmp|var)(/|$)'

echo "[repo-inventory] Scanning repository..." >&2

# Collect file list
mapfile -t files < <(find "$root_dir" -type f \
  ! -path '*/node_modules/*' \
  ! -path '*/dist/*' \
  ! -path '*/.git/*' \
  | sed 's|^./||' | sort)

total_files=${#files[@]}
code_files=0
asset_files=0
content_files=0
config_files=0
script_files=0
test_files=0
backup_files=0
generated_files=0

declare -A per_dir_count=()
declare -A per_ext_count=()
declare -A reasons=()

classify_file(){
  local f="$1"
  local base="${f##*/}"
  local dir="${f%/*}"
  local ext="${base##*.}"
  [[ "$base" == "$ext" ]] && ext="" # no extension

  # Normalise empty directory (for files at repo root)
  [[ "$dir" == "$f" ]] && dir="."  # if no slash present

  # Ensure ext key placeholder
  [[ -z "$ext" ]] && ext="(none)"

  # Track per-dir and per-ext
  : $((per_dir_count["$dir"]+=1))
  : $((per_ext_count["$ext"]+=1))

  # Generated heuristics
  if grep -qE 'DO NOT EDIT|Generated by|AUTO-GENERATED' "$f" 2>/dev/null; then
    ((generated_files++))
  fi

  # Backup / stray heuristics
  if [[ "$base" =~ (~|\.bak|\.old|\.orig)$ ]]; then
    ((backup_files++))
    reasons["$f"]+="backup_pattern;"
  fi

  # Classification buckets
  if [[ "$f" =~ $EXCLUDE_DIRS_REGEX ]]; then
    return 0
  fi

  if [[ "$f" =~ (^|/)tests?/ || "$f" =~ (_|\.)test\.(js|ts|mjs|mts|cjs|jsx|tsx)$ ]]; then
    ((test_files++))
    return 0
  fi

  case "$ext" in
    js|ts|tsx|jsx|astro|mjs|cjs|mts) ((code_files++)) ;;
    sh|bash) ((script_files++)) ;;
    md|markdown|mdx) ((content_files++)) ;;
    json|yml|yaml|toml|lock|env|config|cjs|mjs) ((config_files++)) ;;
    css|scss|sass|pcss) ((code_files++)) ;;
    png|jpg|jpeg|gif|webp|svg|ico|avif) ((asset_files++)) ;;
    *) : ;; # unclassified
  esac
}

echo "[repo-inventory] Classifying $total_files files..." >&2

# Temporarily disable 'exit on error' inside large loop to avoid unexpected termination
set +e
idx=0
for f in "${files[@]}"; do
  classify_file "$f" || echo "[repo-inventory] warn: classification issue on $f" >&2
  ((idx++))
  if (( idx % 200 == 0 )); then
    echo "[repo-inventory] Processed $idx / $total_files files" >&2
  fi
done
set -e
echo "[repo-inventory] Classification complete" >&2

# Identify top noisy directories (potential consolidation)
TOP_DIRS_JSON=$(printf '%s\n' "${!per_dir_count[@]}" | awk -F/ '{print NF-1, $0}' | sort -nr | awk '{print $2}' \
  | while read -r d; do echo -e "${per_dir_count[$d]}\t$d"; done | sort -nr | head -20 \
  | awk 'BEGIN{print "["}{printf (NR>1?",":""); printf "{\"dir\":\""$2"\",\"files\":"$1"}"}END{print "]"}')

TOP_EXT_JSON=$(printf '%s\n' "${!per_ext_count[@]}" | while read -r e; do c=${per_ext_count[$e]}; [[ -z "$e" ]] && e="(none)"; echo -e "$c\t$e"; done \
  | sort -nr | head -20 | awk 'BEGIN{print "["}{printf (NR>1?",":""); printf "{\"ext\":\""$2"\",\"files\":"$1"}"}END{print "]"}')

# Cleanup candidates (backup pattern, generated, deep nesting)
cleanup_candidates=()
for f in "${!reasons[@]}"; do
  cleanup_candidates+=("$f")
done

generate_json(){
  cat <<JSON
{
  "module": "repo_inventory",
  "timestamp": "$(date -Iseconds)",
  "summary": {
    "total_files": $total_files,
    "code_files": $code_files,
    "script_files": $script_files,
    "content_files": $content_files,
    "config_files": $config_files,
    "asset_files": $asset_files,
    "test_files": $test_files,
    "backup_files": $backup_files,
    "generated_files": $generated_files
  },
  "top_directories": $TOP_DIRS_JSON,
  "top_extensions": $TOP_EXT_JSON,
  "cleanup_candidates": [
$(for i in "${!cleanup_candidates[@]}"; do f="${cleanup_candidates[$i]}"; printf '    {"file": "%s", "reasons": "%s"}' "$f" "${reasons[$f]}"; [[ $i -lt $((${#cleanup_candidates[@]}-1)) ]] && printf ',\n'; done)
  ]
}
JSON
}

generate_md(){
  cat <<MD
# Repository Inventory Report

Generated: $(date -Iseconds)

| Metric | Count |
| ------ | -----:|
| Total files | $total_files |
| Code files | $code_files |
| Script files | $script_files |
| Content (Markdown) | $content_files |
| Config | $config_files |
| Assets | $asset_files |
| Tests | $test_files |
| Backups (.bak/~) | $backup_files |
| Generated (heuristic) | $generated_files |

## Top Directories
$(echo "$TOP_DIRS_JSON" | jq -r '.[] | "- " + .dir + " (" + (.files|tostring) + " files)"')

## Top Extensions
$(echo "$TOP_EXT_JSON" | jq -r '.[] | "- ." + .ext + " (" + (.files|tostring) + " files)"')

## Cleanup Candidates
$( if [[ ${#cleanup_candidates[@]} -eq 0 ]]; then echo "(none)"; else
    for f in "${cleanup_candidates[@]}"; do echo "- $f â€” ${reasons[$f]}"; done; fi )

## Suggested Upstream Actions
1. Remove or archive backup variants (e.g. *.bak) after confirming they are obsolete.
2. Consolidate scattered config into a /config or root-level systematic structure.
3. Migrate ad-hoc scripts in /scripts that wrap validation into hunters where appropriate (single measurement layer).
4. Introduce a /infrastructure or /tooling folder if more than 5 generated/build helper scripts cluster.
5. Add CI guard to fail on new *.bak files entering the repo.
6. Convert large one-off Markdown procedural logs into summarized debriefs and archive originals.

*This hunter supports upstream cleanup by making structural debt visible before acting.*
MD
}

generate_json > "$JSON_OUT"
generate_md > "$MD_OUT"

echo "[repo-inventory] Report written to $JSON_OUT and $MD_OUT" >&2
exit 0
